import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torchvision.utils import save_image, make_grid
from PIL import Image
import matplotlib.pyplot as plt

# Define a custom dataset to load images
class ImageDataset(Dataset):
    def __init__(self, image_folder, transform=None):
        self.image_folder = image_folder
        self.transform = transform
        self.image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('png', 'jpg', 'jpeg', 'gif', 'bmp'))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_folder, self.image_files[idx])
        img = Image.open(img_path).convert('RGB')
        
        if self.transform:
            img = self.transform(img)
        
        return img

# Define the Generator model
class Generator(nn.Module):
    def __init__(self, z_dim):
        super(Generator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(z_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 3 * 64 * 64),
            nn.Tanh()  # Output a tensor of shape (3, 64, 64)
        )

    def forward(self, z):
        return self.fc(z).view(-1, 3, 64, 64)

# Define the Discriminator model
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2)
        )
        self.fc = nn.Sequential(
            nn.Linear(512 * 4 * 4, 1),
            nn.Sigmoid()  # Output a probability (0-1) for real/fake
        )

    def forward(self, img):
        x = self.conv(img)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        return self.fc(x)

# Initialize the models, loss function, and optimizers
def initialize_models(z_dim):
    generator = Generator(z_dim)
    discriminator = Discriminator()
    criterion = nn.BCELoss()
    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    return generator, discriminator, criterion, optimizer_g, optimizer_d


# Train the GAN with early stopping
def train_gan(generator, discriminator, dataloader, criterion, optimizer_g, optimizer_d, device, epochs, z_dim, patience=10):
    real_label = 1
    fake_label = 0

    # Lists to store losses for plotting
    d_losses = []
    g_losses = []

    # Variables for early stopping
    best_g_loss = float('inf')
    epochs_without_improvement = 0

    for epoch in range(epochs):
        epoch_d_loss = 0
        epoch_g_loss = 0
        for i, imgs in enumerate(dataloader):
            imgs = imgs.to(device)
            batch_size = imgs.size(0)
            valid = torch.ones(batch_size, 1).to(device) * real_label
            fake = torch.zeros(batch_size, 1).to(device) * fake_label

            # Train the discriminator
            optimizer_d.zero_grad()
            z = torch.randn(batch_size, z_dim).to(device)
            gen_imgs = generator(z)

            real_loss = criterion(discriminator(imgs), valid)
            fake_loss = criterion(discriminator(gen_imgs.detach()), fake)
            d_loss = real_loss + fake_loss
            d_loss.backward()
            optimizer_d.step()
            epoch_d_loss += d_loss.item()

            # Train the generator
            optimizer_g.zero_grad()
            g_loss = criterion(discriminator(gen_imgs), valid)
            g_loss.backward()
            optimizer_g.step()
            epoch_g_loss += g_loss.item()

        # Average losses for the epoch
        epoch_d_loss /= len(dataloader)
        epoch_g_loss /= len(dataloader)
        d_losses.append(epoch_d_loss)
        g_losses.append(epoch_g_loss)

        print(f"[Epoch {epoch}/{epochs}] [D loss: {epoch_d_loss:.4f}] [G loss: {epoch_g_loss:.4f}]")

        # Save images after each epoch
        if epoch % 10 == 0:
            save_image(gen_imgs.data[:2], f"generated_images_epoch_{epoch}.png", nrow=2, normalize=True)

        # Early stopping: Check if generator loss improves
        if epoch_g_loss < best_g_loss:
            best_g_loss = epoch_g_loss
            epochs_without_improvement = 0  # Reset counter
        else:
            epochs_without_improvement += 1

        if epochs_without_improvement >= patience:
            print(f"Early stopping triggered after {epoch + 1} epochs. No improvement in generator loss for {patience} epochs.")
            break

    # Plot losses after training
    plt.figure(figsize=(10, 5))
    plt.plot(d_losses, label="Discriminator Loss")
    plt.plot(g_losses, label="Generator Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("GAN Training Losses")
    plt.show()


# Function to generate and display synthetic images
def generate_and_display_images(generator, z_dim, device, num_images=1):
    generator.eval()
    z = torch.randn(num_images, z_dim).to(device)
    with torch.no_grad():
        generated_images = generator(z)
    
    generated_images = (generated_images + 1) / 2  # Rescale from [-1, 1] to [0, 1]
    grid = make_grid(generated_images, nrow=num_images, normalize=True)
    plt.figure(figsize=(8, 8))
    plt.imshow(grid.permute(1, 2, 0).cpu())
    plt.axis('off')
    plt.show()

# Main function
def main():
    image_folder = ''  # Path to the image folder
    z_dim = 100  # Latent space dimension (random noise vector)
    batch_size = 32
    epochs = 1000
    # Transformations to apply to images (resize, normalize, etc.)
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
    ])

    # Load dataset
    dataset = ImageDataset(image_folder, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Initialize the models, optimizers, and loss function
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    generator, discriminator, criterion, optimizer_g, optimizer_d = initialize_models(z_dim)
    generator.to(device)
    discriminator.to(device)

    # Train the GAN
    train_gan(generator, discriminator, dataloader, criterion, optimizer_g, optimizer_d, device, epochs, z_dim)

    # Generate and display synthetic images
    generate_and_display_images(generator, z_dim, device, num_images=1)

if __name__ == "__main__":
    main()
